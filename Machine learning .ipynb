{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3 align=\"center\"> <font size=\"6\"> Machine learning notebook </h3>\n",
    "    \n",
    " **Table of content:**\n",
    "* [1. Machine learning](#1.)\n",
    "    * [1.2. Types of machine learning](#1.2.)\n",
    "    * [1.3. Machine Learning vs Deep Learning vs AI](#1.3.)\n",
    "    * [1.4. Machine learning algorithms](#1.4.)\n",
    "        * [1.4.1. Linear regression](#1.4.1.)\n",
    "            * [1.4.1.1. Multiple linear regression](#1.4.1.1.)\n",
    "        * [1.4.2. Decision Trees](#1.4.2.)\n",
    "        * [1.4.3. Support vector machine](#1.4.3.)\n",
    "        * [1.4.4. K Nearest Neighbors](#1.4.4.)\n",
    "        * [1.4.5. Naive Bayes](#1.4.5.)\n",
    "        * [1.4.6. Decision Tree Algorithm](#1.4.6.)\n",
    "        * [1.4.7. Random Forest Algorithm](#1.4.7.)\n",
    "        * [1.4.8. K Means Clustering](#1.4.8.)\n",
    "        * [1.4.9. Logistic regression](#1.4.9.)\n",
    "* [2. Clustering](#2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Machine learning <a class=\"anchor\" id=\"1.\"></a>\n",
    "\n",
    "## What is machine learning?\n",
    "\n",
    "Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. \n",
    "\n",
    "In other words, ML is a techique which uses statistical method, enabling machines to learn from their past data, predicts whats going to come next and improves its self.\n",
    "\n",
    "Also, ML is the science of making computers learn and act like humans by feeding data and information without being explicitly programmed.\n",
    "\n",
    "ML is a subset of artificial inteligence:\n",
    "\n",
    "<img src=\"Pictures/8.png\">\n",
    "\n",
    "\n",
    "**How ML works?**\n",
    "\n",
    "- Phase 1: Learning\n",
    "<img src=\"Pictures/52.png\">\n",
    "\n",
    "- Phase 2: Prediction\n",
    "<img src=\"Pictures/53.png\">\n",
    "\n",
    "\n",
    "**Machine Learning Workflow**\n",
    "1. Define objective\n",
    "2. Prepare the data\n",
    "    - Cleaning the data, understand what is coming in, then you know what tool to use\n",
    "3. Collect the data\n",
    "4. Select Algorith\n",
    "5. Train the model\n",
    "6. Test the model\n",
    "    - test might come incorrect so you have to change how you're training the model\n",
    "    - If the traning of the model doesn't solve the problem check the selected algorithm and change it  \n",
    "7. Predict\n",
    "\n",
    "\n",
    "\n",
    "**Steps of ML:**\n",
    "\n",
    "<img src=\"Pictures/9.png\">\n",
    "\n",
    "## Machine Learning\n",
    "- The system is able to make predictions or take decisions based on past data\n",
    "    - Better decisions and predictions\n",
    "    - Quick and accurate outcomes\n",
    "    - More powerful processing capablity\n",
    "    - Analyzing complex big data\n",
    "    - Managing large amounts of data\n",
    "    - Inexpensive\n",
    "\n",
    "## Application of machine learning\n",
    "\n",
    "- Search engine results\n",
    "- Voice recognition\n",
    "- Number plate recognition\n",
    "- Dream reader\n",
    "\n",
    "\n",
    "- autonomous cars\n",
    "- fraud detections\n",
    "- disease detection\n",
    "- voice and face recognition\n",
    "- spam detection /engagement bait \n",
    "\n",
    "\n",
    "\n",
    "**Basic divisions what ML does:**\n",
    "- Classification\n",
    "    - predicting of a category (yes/no, 0/1)\n",
    "    - example : will the stock price increase or decrease\n",
    "- Regression\n",
    "    - prediction of a quantity\n",
    "    - predicting the age of a person based on the height, health and other factors\n",
    "- Anomaly detection\n",
    "    - detection of a anomaly\n",
    "    - detect money withdrawal anomalies / stock exchage anomaly\n",
    "- Clustering\n",
    "    - discovering structure in unexplored/unknown data\n",
    "    - finding groups of customers with similar behavior given a large database of customer data containing their demographics and past buying records\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "## Visualisation of machine learning process:\n",
    "\n",
    "<img src=\"Pictures/1.png\">\n",
    "\n",
    "<img src=\"Pictures/56.png\">\n",
    "\n",
    "# 1.2. Types of machine learning <a class=\"anchor\" id=\"1.2.\"></a>\n",
    "\n",
    "1. Supervised learning\n",
    "2. Unsupervised learning\n",
    "3. Reinforcement learning\n",
    "\n",
    "## 1. Supervised learning\n",
    "\n",
    "- Uses labeled data to teach the model\n",
    "- ML model learns from the past input data and makes future prediction as output\n",
    "- we give machine samples so that machine can learn from it\n",
    "- Classification and regression are the two most common types of supervised learning\n",
    "- Systems are able to predict future outcomes based on past data\n",
    "- Requires both an input and output to be given to the model for it to be trained\n",
    "\n",
    "**Types of supervised learning**\n",
    "\n",
    "Two major types:\n",
    "- **Classification** \n",
    "    - is concered with building models that separate data into distinct classes\n",
    "    - Used when the output is categorical like \"YES\" or \"NO\"\n",
    "    - Algorithms used:\n",
    "        - Decision tree\n",
    "        - Support Vector Machine\n",
    "        - Naive Bayes\n",
    "        - Random Forest\n",
    "        - Logistic regression\n",
    "        - K Nearest Neighbors (KNN)\n",
    "- **Regression** \n",
    "    - based on previos input data, the machine predicts continuous output value\n",
    "    - Used when value need to be predicted like the \"stock prices\"\n",
    "    - Algorithms used:\n",
    "        - Simple linear regression\n",
    "            - When we have a lot of data and we draw a line through it\n",
    "        - Multiple linear regression\n",
    "            - We have multiple variables\n",
    "        - Polynomial regression\n",
    "            - Drawing a curved line through data\n",
    "        \n",
    "**Most popular algorithms:**\n",
    "\n",
    "<img src=\"Pictures/3.png\">\n",
    "\n",
    "**Supervised learning visualisation:**\n",
    "<img src=\"Pictures/2.png\">\n",
    "\n",
    "<img src=\"Pictures/10.png\">\n",
    "\n",
    "## 2. Unsupervised learning\n",
    "\n",
    "- ML model uses unlabeled input data and allows the algorithm to act on that information without guidance\n",
    "- Learning with unlabeled dana\n",
    "- No predetermined result in advance. Learning through exploration\n",
    "- Clustering and Association are the two most common types of unsupervised learning\n",
    "- Systems are able to identify hidden patterns from the input data provided (unlabeled data)\n",
    "- By making the data more readable and organized, the patterns, similarities, or anomalies become more evident\n",
    "\n",
    "<img src=\"Pictures/54.png\">\n",
    "\n",
    "**Types of supervised learning**\n",
    "\n",
    "Two major types:\n",
    "- **Clustering** \n",
    "    - is used for analyzing and grouping data which does not include pre-labeled calss or even a class atribute at all\n",
    "    - Used when the data needs to be organised to find patterns in the case of \"product recommendation\"\n",
    "    - Algorithms used:\n",
    "        - K-means\n",
    "        - Hierarchical Clustering\n",
    "        - Hidden Markov model\n",
    "        - \n",
    "- **Regression** \n",
    "    - discovers the probability of the co-occurrence of items in a collection\n",
    "    - Algorithms used:\n",
    "        - Apriori algorith\n",
    "        - FP-Growth\n",
    "\n",
    "\n",
    "**Most popular algorithms:**\n",
    "<img src=\"Pictures/5.png\">\n",
    "\n",
    "**Unsupervised learning visualisation:**\n",
    "<img src=\"Pictures/4.png\">\n",
    "\n",
    "## 3. Reinforcement Learning\n",
    "- Reinforcement learning involves teaching the machine to thing for itself based on it's past action reward\n",
    "- Systems are given no training\n",
    "- It learns on the basis of the reward/punishment it recieved for performing its last action\n",
    "- learning how to learn\n",
    "- agent learns how to behave in an enviroment by performing actions and seeing the results\n",
    "- It helps increase the efficiency of a tool/function or a program\n",
    "- similar learning pattern as a humas (trial and error)\n",
    "    \n",
    "**Most commonly used algorithms:**\n",
    "- Q-learning\n",
    "- Temporal difference (TD)\n",
    "- Deep adversarial networks\n",
    "\n",
    "**Reinforcement learning visualisation:**\n",
    "<img src=\"Pictures/11.png\">\n",
    "\n",
    "## Supervised VS unsupervised ML\n",
    "\n",
    "- can be  combined - you have image and not sure what you're looking for(unstructured data), first find all things connected together, then somebody labeles that data. Now you can use labeled data to predict\n",
    "\n",
    "<img src=\"Pictures/12.png\">\n",
    "\n",
    "## Some Applications of ML\n",
    "- Instance Segmentation\n",
    "- Number Plate Detection\n",
    "- Automatic Translation\n",
    "- ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. Machine Learning vs Deep Learning vs AI <a class=\"anchor\" id=\"1.3.\"></a>\n",
    "\n",
    "\n",
    "## Humans vs AI\n",
    "- **Human Intelligence**\n",
    "    - Use available information to take decisions\n",
    "    - Communicate with people\n",
    "    - Identify patterns in data\n",
    "    - Remember what people have said\n",
    "    - Adapt to new situations\n",
    "- **Artificial Intelligence**\n",
    "    - AI develops computer systems that can accomplish tasks that require human inteligence\n",
    "    - Interact with humans using their natural language\n",
    "    - Provides more accurate results\n",
    "    - Learns from their mistakes and adapt to new environments\n",
    "    - Learns from the data and automates repetitive learning\n",
    "    \n",
    "## Deep Learning\n",
    "- Systems think and learn like humans using artificial neaural networks\n",
    "    - Performance improves with more data\n",
    "    - Better scalability\n",
    "    - Problem solved in an end-to-end method\n",
    "    - Best features are selected by the system\n",
    "    - Is a subset of ML\n",
    "    - Lesser testing time\n",
    "    \n",
    "## Real-life Examples\n",
    "**Artificial Intelligence**\n",
    "- News generation\n",
    "- Smart Home Devices (e.g. Amazon Echo)\n",
    "\n",
    "**Machine Learning**\n",
    "- Spam detection\n",
    "- Search engine result refining\n",
    "\n",
    "**Deep Learning**\n",
    "- Automatic translation\n",
    "- Chatbots\n",
    "\n",
    "\n",
    "## Types of AI\n",
    "**1. Reactive Machines**\n",
    "- Systems that only react\n",
    "- Don't form memories\n",
    "- Doesn't use any past experiences for new decisions\n",
    "    \n",
    "**2. Limited Memory**\n",
    "- Systems look into the past\n",
    "- Information is added over a period of time\n",
    "- Information is short lived\n",
    "    \n",
    "**3. Theory of Mind**\n",
    "- Systems being able to understand human emotions and how they effect decision making\n",
    "- To adjust their behavious according to their human understanding\n",
    "    \n",
    "**4. Self-awareness**\n",
    "- Systems being aware of themselves\n",
    "- Understand their own internal states\n",
    "- Predicting other people's geeling and act appropriately\n",
    "    \n",
    "    \n",
    "\n",
    "## Comparing ML and DL\n",
    "**Machine Learning**\n",
    "- Enables machines to take decisions on their own, based on the past data\n",
    "- Needs only a small amount of training data\n",
    "- Works well on low-end systems\n",
    "- Most features need to be identified in advance and manually coded\n",
    "- The problem is divided into parts and solved individualy and then combined\n",
    "- Testing takes longer\n",
    "- Crisp rules explain why a certain decision was taken\n",
    "\n",
    "**Deep Learning**\n",
    "- Enables machines to take decisions with the help of artificial neural networks\n",
    "- Needs a large amount of training data\n",
    "- Needs high end systems to work\n",
    "- The machine learns the features from the data it is provided \n",
    "- The problem is solved in an end-to-end manner\n",
    "- Testing takes less time\n",
    "- Since the system takes decisions based on it's own logic, the reasoning may be difficult to interpret\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. Machine learning algorithms <a class=\"anchor\" id=\"1.4.\"></a>\n",
    "\n",
    "## 1.4.1. Linear regression <a class=\"anchor\" id=\"1.4.1.\"></a>\n",
    "- linear regression is one of the most well known and well understood algorithms in statistics and machine learning\n",
    "- Linear regression is a process used for estimating the relationships among variables. Here, one of the variables is dependent on one or more independent variables.\n",
    "    - E.g. weight and height of people\n",
    "- Linear Regression is a statistical model used to predict the relationship between independent and dependent variables by examining two factors:\n",
    "    - Which variables in particular are significant predictiors of the outcome variables?\n",
    "    - How significant is the Regression line to make predictions with highest possible accuracy\n",
    "    \n",
    "- The simplest form of a simple linear regression equation with one dependent and one independent variable is represented by:\n",
    "\n",
    "$$ \\large y = mx + c $$\n",
    "\n",
    "<img src=\"Pictures/61.png\">\n",
    "    \n",
    "### Independent & Dependent Variables\n",
    "- Example: Based on the amount of rainfall, how much would be the crop yield?\n",
    "\n",
    "<img src=\"Pictures/59.png\">\n",
    "\n",
    "- linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and a single output variable (y)\n",
    "<img src=\"Pictures/16.png\">)\n",
    "- mimimizing the distance \n",
    "    - moving line trough the data points to make sure the best fit line has least square distance between the data points and the regression line\n",
    "    - there are lots of ways to minimize the distance between the line and the data points like **Sum of squared errors, Sum of absoulute errors, Root mean square error etc.**\n",
    "\n",
    "\n",
    "### Numerical & Categorical Values\n",
    "\n",
    "<img src=\"Pictures/60.png\">\n",
    "\n",
    "\n",
    "### Applications of Linear Regression\n",
    "- Economic Growth\n",
    "    - used to determine economic growth of a country or a state in the coming quarter\n",
    "    - can also be used to predict the GDP of a country\n",
    "- Product Price\n",
    "    - can be used to predict what would be the price of a product in the future\n",
    "- Housing Sales\n",
    "    - to estimate the number of houses a builder would sell and at what price in the coming months\n",
    "- Score Prediction\n",
    "    - to predict the number of runs a player would score in the coming matches based on previous performance\n",
    "    \n",
    "## Mathematical implementation of Linear regression\n",
    "\n",
    "<img src=\"Pictures/17.png\">\n",
    "**Xi and Yi are the mean values of X and Y. The line goes through that point. Now it's time to calculate m.**\n",
    "<img src=\"Pictures/20.png\">\n",
    "\n",
    "**Now we can calculate the value of c. Point c is the staeting point of the line on y axis**\n",
    "<img src=\"Pictures/22.png\">\n",
    "<img src=\"Pictures/21.png\">\n",
    "\n",
    "**Predicting the values of y using x =(1,2,3,4,5) and ploting the points**\n",
    "- Calculating what Y thinks they are, not what they actualy are. \n",
    "- Yp = Predicted values of Y (of what we think its going to be when we plug those numbers in the plot)\n",
    "<img src=\"Pictures/23.png\">\n",
    "<img src=\"Pictures/24.png\">  \n",
    "    \n",
    "    \n",
    "### Intuition behind the Regression line\n",
    "- Lets consider a sample dataset with 5 rows and find out how to draw the regression line:\n",
    "\n",
    "<img src=\"Pictures/62.png\">\n",
    "\n",
    "\n",
    "- If we go ahead and plot those point on a graph we can see how a line would fir perfectly through the middle:\n",
    "\n",
    "<img src=\"Pictures/63.png\">\n",
    "\n",
    "- The next we want to know is what the *mean* is:\n",
    "    - **3** for the X variable\n",
    "    - **4** for the Y variable\n",
    "- If we plot the means on the graph it makes a nice line through the middle:\n",
    "\n",
    "<img src=\"Pictures/64.png\">\n",
    "\n",
    "\n",
    "- From that point we can calculate the relevant features in the dataset: $ X^2, Y^2 and XY $, along with their sums:\n",
    "\n",
    "<img src=\"Pictures/65.png\">\n",
    "\n",
    "\n",
    "- And we can get the formulas for $ m $ and $ c $:\n",
    "\n",
    "<img src=\"Pictures/66.png\">\n",
    "\n",
    "\n",
    "- **n** = number of data\n",
    "\n",
    "- Now we can go and find out the predicted values of Y for corresponding values of X using the linear equation where **m = 0.6** and **c = 2.2**\n",
    "- The blue points represent the **actual Y values** and the brown points represent the predicted Y values\n",
    "- The distance between the actual and predicted values are known as **residuals or errors**\n",
    "- The best fit line should have the least sum of squares of there errors also known as **e square**\n",
    "\n",
    "<img src=\"Pictures/67.png\">\n",
    "\n",
    "- The sum of squared errors for this regression line is 2.4\n",
    "- We check this error for each line and conclude the best fir line having the least e square value\n",
    "\n",
    "<img src=\"Pictures/68.png\">\n",
    "\n",
    "- We keep mobing this line through the data points to make sure the Best fir line has the least square distance between the data points and the regression line\n",
    "- There are a lots of ways to minimize the distance between the line and the data points like: Sum of Squared Errors, Sum of Absolute Errors, Root Mean Square Error...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Example 1. Variables having positive and negative relationships\n",
    "\n",
    "<img src=\"Pictures/13.png\">\n",
    "\n",
    "<img src=\"Pictures/14 .png\">\n",
    "<img src=\"Pictures/15.png\">\n",
    "\n",
    "## 1.4.1.1. Multiple linear regression <a class=\"anchor\" id=\"1.4.1.1.\"></a>\n",
    "\n",
    "\n",
    "<img src=\"Pictures/69.png\">\n",
    "\n",
    "\n",
    "- Multiple variables coming in\n",
    "- Instead of $ x $ we have $ x_1, x_2, x_3 $ etc\n",
    "- Instead of having one slope, each variable has it's own slope attached to it - $ m_1, m_2, m_3 $ etc\n",
    "\n",
    "### Multiple Linear Regression Implementation\n",
    "- E.g. analyzing a companies sectors to predict the profit of the company.\n",
    "- Instead of looking just at R&D we'll look at multiple features:\n",
    "\n",
    "<img src=\"Pictures/70.png\">\n",
    "\n",
    "\n",
    "- From there we'll try to predict what the profit would be\n",
    "\n",
    "## 1.4.2. Decision Trees <a class=\"anchor\" id=\"1.4.2.\"></a>\n",
    "\n",
    "- Decision Tree is a tree shaped algorithm used to determine a course of action. \n",
    "- Each branch of the tree represents a possible decision, occurrence or reaction\n",
    "\n",
    "\n",
    "\n",
    "# Example 1. Determining wheter to play or not to play golf\n",
    "\n",
    "<img src=\"Pictures/46.png\">\n",
    "\n",
    "- our data are : \n",
    "    - Outlook (Rainy, Sunny, Overcast)\n",
    "    - Temperature (hot, mild, cool)\n",
    "    - Humidity (high, normal)\n",
    "    - Windy (fals, true)\n",
    "\n",
    "<img src=\"Pictures/25.png\">\n",
    "\n",
    "**How to decide how to split the data up? Is this the right decision tree?**\n",
    "For that we need to calculate Entropy and information gain\n",
    "\n",
    "<img src=\"Pictures/26.png\">\n",
    "\n",
    "**Entropy**\n",
    "- is the measure od randomness or \"impurity\" in the dataset\n",
    "- the lower number the better\n",
    "- we can calculate Entropy of target class of the data set (whole entropy). Target class of the data set is should we play golf or not\n",
    "- we can also calculate the entropy for playing gold and the outlook. \n",
    "\n",
    "$$ Entropy = I(p, n) = - \\frac{p}{p + n} * \\log_{2} (\\frac{p}{p + n}) - \\frac{n}{p + n} * \\log_{2} (\\frac{n}{p + n}) $$\n",
    "\n",
    "$ p $ = probability that we would play the game of golf<br>\n",
    "$ n $ = probability that we would not play the game of golf<br>\n",
    "\n",
    "- In the above dataset there are 14 data points\n",
    "    - 5 state that we shouldn't play golf\n",
    "    - 9 state that we should\n",
    "- So it's easy to calculate entropy:\n",
    "\n",
    "$$ \\large E(5, 9) $$<br>\n",
    "$$ \\large I(5/14, 9/14) $$<br>\n",
    "$$ \\large I(0.36, 0.64) $$<br>\n",
    "$$ \\large = - (0.36 \\log_{2} 0.36) - (0.64 \\log_{2} 0.64) $$<br>\n",
    "$$ \\large = 0.94 $$<br>\n",
    "\n",
    "\n",
    "Similary, we can calculate the entropy of other predictors like Temperature, Humidity, Windy\n",
    "<img src=\"Pictures/27.png\">\n",
    "\n",
    "**Information gain**\n",
    "- The greates number is the greatest gain of information and that is the split we want\n",
    "- \n",
    "<img src=\"Pictures/28.png\">\n",
    "\n",
    "**Outlook is our greatest information gain and that is our fist split.** \n",
    "- Now we build the the new decision tree\n",
    "- Continue down the tree using the next greatest information gain \n",
    "<img src=\"Pictures/29.png\">\n",
    "\n",
    "## 1.4.3. Support vector machine (SVM) <a class=\"anchor\" id=\"1.4.3.\"></a>\n",
    "\n",
    "- Support vector machine is a widely used classification algorithm\n",
    "- Supervised learning, mostly used for classification, but can be used for regression\n",
    "- the idea of support vector machine is simple: The algorithm creates a separation line which divides the vlasses in the best possible manner\n",
    "    - for example = dog or car, disease or no disease\n",
    "- The goal of SVM is to choose a **hyperplane** with the greatest possible margin between the decision line and the nearest point within the training set\n",
    "    - **hyperplane** - is a multidimensional cut in the data\n",
    "- **distance margin =** The distance between the hyperplane and the nearest data point from either set\n",
    "\n",
    "- Suppose that we have labeled sample data, which tells height and weight of males and females:\n",
    "\n",
    "<img src=\"Pictures/47.png\">\n",
    "\n",
    "- A new data point arrives, and we want to know how can a machine classify whether a new data point is male or a female?\n",
    "\n",
    "<img src=\"Pictures/48.png\">\n",
    "\n",
    "- The goal is to choose a hyperplane with the greatest possible margin betwwen the decision line and the nearest point within the training set\n",
    "    - **Distance margin** - distance between the hyperplane and the nearest data point from either set\n",
    "    \n",
    "<img src=\"Pictures/49.png\">\n",
    "\n",
    "- When we draw the hyperplanes, we observe that Line 1 has the maximum distance margin so it will classify the new data point correctly\n",
    "- As the result, the new data point is **male**\n",
    "\n",
    "<img src=\"Pictures/50.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Why SVM?\n",
    "- SVM is a supervised learning method that looks at data and sorts in into one of the two categories\n",
    "- But how does the prediction work?\n",
    "    - Let's look at an example\n",
    "    \n",
    "<img src=\"Pictures/109.png\">\n",
    "    \n",
    "- We draw a line between two groups and we can predict the outcome based on the where new data points falls into\n",
    "\n",
    "\n",
    "#### Example\n",
    "- We are given a set of people with different height and weight\n",
    "\n",
    "<img src=\"Pictures/110.png\">\n",
    "    \n",
    "- We can plot this data to a graph along with a new point that we want to classify:\n",
    "\n",
    "<img src=\"Pictures/111.png\">\n",
    "    \n",
    "- To classify the new point, we need to split the data and we can do that with any of these two lines:\n",
    "\n",
    "<img src=\"Pictures/112.png\">\n",
    "    \n",
    "- To predict the gender of a new data point we should split the data in the best possible way\n",
    "- This would be the best possible way because this line has the maximum space that separates the two classes:\n",
    "\n",
    "<img src=\"Pictures/113.png\">\n",
    "    \n",
    "- Before we go any futher, we need to add some technical terms into this\n",
    "\n",
    "<img src=\"Pictures/114.png\">\n",
    "    \n",
    "- In tehnical terms, we can say that the distance between the support vecror and the hyperplane should be as far as possible (hyperplane: full line, support vectors: dashed lines)\n",
    "- **D+** - the shortes distance to the closest positive point\n",
    "- **D-** - the shortest distance to the closest negative point\n",
    "- Sum of D+ and D- is called the distance margin\n",
    "- From the distance margin we can create an optimal hyperplane\n",
    "- Based on the hyperplane, we can say that the new data point belongs to male gender\n",
    "- If we select a hyperplane having low margin then there is a high chance of misclassification\n",
    "\n",
    "\n",
    "- In some cases the dataset might look like this\n",
    "    - It's needed to use the Kernel Function which will take the 1-D input and transfer it to 2-D output\n",
    "    \n",
    "<img src=\"Pictures/115.png\">\n",
    "    \n",
    "- When Kernel Function is executed, it's very easy to draw a line betweeen datasets:\n",
    "\n",
    "<img src=\"Pictures/116.png\">\n",
    "    \n",
    "- But what if we have even more complex dataset, the one that looks like this:\n",
    "\n",
    "<img src=\"Pictures/117.png\">\n",
    "    \n",
    "- In this case Kernel Function will transorm the dataset into 3-D one:\n",
    "\n",
    "<img src=\"Pictures/118.png\">\n",
    "    \n",
    "    \n",
    "    \n",
    "### Advantages of SVM\n",
    "- High dimensional input space\n",
    "- Sparse document vectors\n",
    "- Regularization parameter\n",
    "\n",
    "\n",
    "## 1.4.4. K Nearest Neighbors <a class=\"anchor\" id=\"1.4.4.\"></a>\n",
    "\n",
    "- K nearest neighbors alghorithm works in a way that a new data point is assigned to a neighboring group it is most similar to\n",
    "- In K nearest Neighbors, K can be an integer greater than 1. So, for every new data point we want to classify, we compute to which neighboring group it is closest to\n",
    "- **K** is the number of nearest neighboring data points we wish to compare the unknown data with\n",
    "\n",
    "<img src=\"Pictures/58.png\">\n",
    "\n",
    "- Because KNN is based on feature similarity, we can do classification using KNN Classifier\n",
    "- K Nearest Neighbors is one of the simplest **Supervised** ML algorithms mostly used for classification\n",
    "- It classifies a data point based on how its neighbors are classified\n",
    "- Stores all available cases and classifies new cases based on a similarity measure\n",
    "- *k* in KNN is a parameter that refers to the number of nearest neighbors to include in the majority voting process, for example:\n",
    "\n",
    "<img src=\"Pictures/102.png\">\n",
    "    \n",
    "- A data point is classified by majority votes from its 5 nearest neighbors\n",
    "- Here the unknown point would be classified as red, since 4 out of 5 neighbors are red\n",
    "\n",
    "\n",
    "### How do we choose \"k\"?\n",
    "- KNN algorithm is based on **feature similarity**: Choosing the right value of k is a process called parameter tuning, and is important for better accuracy\n",
    "\n",
    "<img src=\"Pictures/103.png\">\n",
    "    \n",
    "- So at `k=3` we can classify `?` as a square\n",
    "- One way of choosing the right k value is to:\n",
    "    - Sqrt(n), where n is the total number of data points\n",
    "    - Odd value of K is selected to avoid confusion between two classes of data\n",
    "    \n",
    "    \n",
    "### When do we use KNN Algorithm?\n",
    "- When data is labeled\n",
    "- Data is noise free\n",
    "- Dataset is small\n",
    "    - Because KNN is a \"lazy learner\" i.e. doesn't learn a discriminative function from the training set\n",
    "    \n",
    "    \n",
    "### How does KNN Algorithm work?\n",
    "- Consider a dataset having two variables: height (cm) and weight (kg) and each point is classidied as Normal or Underweight:\n",
    "\n",
    "<img src=\"Pictures/104.png\">\n",
    "    \n",
    "- On the basis of the given data we have to classify the below set as Normal or Underweight using KNN:\n",
    "\n",
    "<img src=\"Pictures/105.png\">\n",
    "\n",
    "- To find the nearest neighbors, we will calculate Euclidean distance\n",
    "    - According to the Euclidean distance formula, the distance between two points in the plane with coordinates (x, y) and (a, b) is given by:\n",
    "    \n",
    "$$ \\large dist(d) = \\sqrt{(x - a)^2 + (y - b)^2} $$\n",
    "\n",
    "- Let's calculate it to understand clearly:\n",
    "\n",
    "<img src=\"Pictures/106.png\">\n",
    "    \n",
    "$$ dist(d1) = \\sqrt{(170 - 167)^2 + (57 - 51)^2} = 6.7 $$<br>\n",
    "$$ dist(d2) = \\sqrt{(170 - 182)^2 + (57 - 62)^2} = 13 $$<br>\n",
    "$$ dist(d3) = \\sqrt{(170 - 176)^2 + (57 - 69)^2} = 13.4 $$<br>\n",
    "\n",
    "- Similarly, we will calculate Euclidean distance of unknown data points from all the points in the dataset\n",
    "- Hence, we have calculated the Euclidean distance of unknown data point from all the points as shown:\n",
    "\n",
    "<img src=\"Pictures/107.png\">\n",
    "    \n",
    "- Where (x1, y1) = (57, 170) whose class we have to classify\n",
    "- Now, let's calculate the nearest neighbor at k=3\n",
    "\n",
    "<img src=\"Pictures/108.png\">\n",
    "    \n",
    "- So, majority of neighbors are pointing towards \"Normal\"\n",
    "    - Hence, as per KNN algorithm the class of (57, 170) should be \"Normal\"\n",
    "    \n",
    "\n",
    "## 1.4.5. Naive Bayes Classifier<a class=\"anchor\" id=\"1.4.5.\"></a>\n",
    "\n",
    "- Mostly used when you need to make a prediction on a very large dataset\n",
    "- **Conditional probability** is the probability of event A happening given that another event B already happened\n",
    "- used in spam detection\n",
    "\n",
    "<img src=\"Pictures/57.png\">\n",
    "\n",
    "### Introducing Bayes Theorem\n",
    "- Naive Bayes Classifier works on the principles of conditional probability as given by the Bayes Theorem\n",
    "- Let's go through some of the simple concepts in probability that we will be using\n",
    "- Let us consider the following example of tossing 2 coins\n",
    "    - Here, the sample space is\n",
    "    $ \\{HH, HT, TH, TT\\} $\n",
    "    1. P(Getting two heads) = 1/4\n",
    "    2. P(At least one tail) ) 3/4\n",
    "    3. P(Second coin being had given first coin is tail) = 1/2\n",
    "    4. P(Getting two heads given first coin is head) = 1/2\n",
    "    \n",
    "- Bayes Theorem gives the conditional probability of an event A given another event B has occurred\n",
    "\n",
    "$$ \\large P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $$\n",
    "\n",
    "Where:\n",
    "- $ P(A|B) $ = Conditional probability of A given B\n",
    "- $ P(B|A) $ = Conditional probability of B given A\n",
    "- $ P(A) $ = Probability of event A\n",
    "- $ P(B) $ = Probability of event B\n",
    "\n",
    "- Let us apply Bayes theorem to our example:\n",
    "    - In this sample space, let A be the event that second coin is head and B be the event that first coin is tail\n",
    "    \n",
    "$$ \\large = \\frac{\\frac{1}{2} * \\frac{1}{2}}{\\frac{1}{2}} = \\frac{1}{2} = 0.5 $$\n",
    "\n",
    "- Bayes Theorem basically calculates the conditional probability of the occurrence of an event based on prior knowledge of conditions that might be related to the event\n",
    "\n",
    "\n",
    "### Understanding Naive Bayes and Machine Learning\n",
    "- Supervised Learning - Classification\n",
    "\n",
    "\n",
    "### Where is Naive Bayes used?\n",
    "- Face recognition\n",
    "- Weather prediciton\n",
    "- Medical diagnosis\n",
    "- News classification\n",
    "\n",
    "\n",
    "### Understanding Naive Bayes Classifier\n",
    "- To predict whether a person will purchase a product on a specific combination of Day, Discount and Free Delivery using Naive Bayes Classifier\n",
    "\n",
    "<img src=\"Pictures/120.png\">\n",
    "\n",
    "- Here's how the dataset looks:\n",
    "\n",
    "<img src=\"Pictures/121.png\">\n",
    "\n",
    "- The first thing we'll do is, based on this dataset containing three input types of Day, Discount and Free Delivery, we will populate frequency tables for each attribute:\n",
    "\n",
    "<img src=\"Pictures/122.png\">\n",
    "\n",
    "- Let the event *Buy* be **A**\n",
    "- All the others are **B**\n",
    "- Now let us calculate the likelihood table for one of the variable, Day, which includes Weekday, Weekend and Holiday\n",
    "\n",
    "<img src=\"Pictures/123.png\">\n",
    "\n",
    "$$ \\large P(B) = P(Weekday) = 11/30 = 0.37 $$<br>\n",
    "$$ \\large P(A) = P(No Buy) = 6/30 = 0.2 $$<br>\n",
    "$$ \\large P(B|A) = P(Weekday | No Buy) = 2/6 = 0.33 $$<br>\n",
    "\n",
    "- Based on tihs likelihood table, we will calculate conditional probabilities as below:\n",
    "\n",
    "$$ \\large P(A|B) = P(No Buy | Weekday) = (0.33 * 0.2) / 0.367 = 0.179 $$<br>\n",
    "\n",
    "- As the Probability(Buy | Weekday) is more than Probability(No Buy | Weekday) we can conclude that a customer will most likely buy the product on a Weekday\n",
    "- We can now construct likelihood tables for other 2 variables:\n",
    "\n",
    "<img src=\"Pictures/124.png\">\n",
    "\n",
    "<img src=\"Pictures/125.png\">\n",
    "\n",
    "- Let us use these 3 likelihood tables to calculate whether a customer will purchase a product on a specific combintion of day, discount and free delivery or not\n",
    "- Here, let us take a combination of these factors:\n",
    "    - Day = Holiday\n",
    "    - Discount = Yes\n",
    "    - Free Delivery = Yes\n",
    "\n",
    "Let A = No Buy<br>\n",
    "P(A|B) = P(No Buy | Discount = Yes, Free Delivery = Yes, Day = Holiday) \n",
    "\n",
    "$$ \\large \\frac{\\frac{1}{6} * \\frac{2}{6} \\frac{3}{6} \\frac{6}{30}}{\\frac{20}{30} * \\frac{23}{30} * \\frac{11}{30}} = 0.178 $$\n",
    "\n",
    "\n",
    "Let A = Buy<br>\n",
    "P(A|B) = P(Yes Buy | Discount = Yes, Free Delivery = Yes, Day = Holiday) \n",
    "\n",
    "$$ \\large \\frac{\\frac{19}{24} * \\frac{21}{24} \\frac{8}{24} \\frac{24}{30}}{\\frac{20}{30} * \\frac{23}{30} * \\frac{11}{30}} = 0.986 $$\n",
    "\n",
    "\n",
    "Probability of Purchase = 0.986<br>\n",
    "Probability of no Purchase = 0.178\n",
    "\n",
    "Let us now normalize these probabilities to get the likelihood of the events\n",
    "- Sum of probabilities\n",
    "    - 0.986 + 0.178 = 1.164\n",
    "- Likelihood of **purchase**:\n",
    "    - 0.986 / 1.164 = 84.71%\n",
    "- Likelihood of **no purchase**:\n",
    "    - 0.178 / 1.164 = 15.29%\n",
    "    \n",
    "- As 84.71% is greater than 15.29%, we can conclude that an average customer will buy on a holiday with discount and free delivery\n",
    "\n",
    "\n",
    "### Advantages of Naive Bayes Classifier\n",
    "<img src=\"Pictures/126.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.6. Decision Tree Algorithm<a class=\"anchor\" id=\"1.4.6.\"></a>\n",
    "\n",
    "- Decision tree starts with a problem\n",
    "- Decision tree is a tree shaped diagram used to determine a course of action. Each branch of the tree represents a possible decision, occurrence or reaction\n",
    "\n",
    "\n",
    "### Problems Decision Trees can Solve\n",
    "- **Classification**\n",
    "    - A classification tree will determine a set of logical if-then conditions to classify problems\n",
    "    - For example, disciminating between three types of flowers based on certain features\n",
    "- **Regression**\n",
    "    - Regression tree is used when the target variable is numerical or continious in nature\n",
    "    - We fit a regression model to the target variable using each of the independent variables\n",
    "    - Each split is made based on the sum of squared error\n",
    "    \n",
    "    \n",
    "### Advantages of Decision Tree\n",
    "- Simple to understand, interpret and visualize\n",
    "- Little effort is required for data preparation\n",
    "- Can handle both numerical and categorical data\n",
    "- Non-linear parameters don't effect its performance\n",
    "    - even if data doesn't fit nicely on a graph it can still be used to make predictions\n",
    "    \n",
    "    \n",
    "### Disadvantages of Decision Tree\n",
    "- Overfitting\n",
    "    - occurs when the algorithm captures noise in the data\n",
    "- High variance\n",
    "    - the model can get unstable due to small variation in data\n",
    "- Low biased tree\n",
    "    - a highly complicated Decision tree tends to have a low vias which makes it difficult for the model to work with new data\n",
    "    \n",
    "    \n",
    "### Important Terms\n",
    "**ENTROPY**\n",
    "- Measure of randomness or unpredictability in the dataset\n",
    "\n",
    "**INFORMATION GAIN**\n",
    "- It is the measure of decrease in entropy after the dataset is split\n",
    "\n",
    "<img src=\"Pictures/80.png\">\n",
    "    \n",
    "**LEAF NODE**\n",
    "- Carries the classification or the decision\n",
    "\n",
    "<img src=\"Pictures/81.png\">\n",
    "\n",
    "**ROOT NODE**\n",
    "- The top most decision node is known as the root node\n",
    "\n",
    "<img src=\"Pictures/82.png\">\n",
    "    \n",
    "    \n",
    "### How Does a Decision Tree Work?\n",
    "- Here are some animals:\n",
    "\n",
    "<img src=\"Pictures/83.png\">\n",
    "    \n",
    "- *Problem Statement*: classify the different types of animals based on their features using decision tree\n",
    "- The dataset is looking quite messy and the entropy is high in this case\n",
    "- Let's look at the training dataset:\n",
    "\n",
    "<img src=\"Pictures/84.png\">\n",
    "    \n",
    "- We have to frame the conditions that split the data in such a way that the information gain is the highest\n",
    "    - Gain is the measure of decrease in entropy after splitting\n",
    "- And here's a formula for entropy:\n",
    "\n",
    "$$ \\large \\sum_{i=1}^{k} P(value_i) \\log_2 (P(value_i)) $$\n",
    "\n",
    "- Let's try to calculate the entropy for the current dataset\n",
    "- We have:\n",
    "    - 3 giraffes\n",
    "    - 2 tigers\n",
    "    - 1 monkey\n",
    "    - 2 elephants\n",
    "        - total of 8 animals\n",
    "- If we plug that in to the formula:\n",
    "\n",
    "$$ ENTROPY = (\\frac{3}{8}) \\log_2 (\\frac{3}{8}) + (\\frac{2}{8}) \\log_2 (\\frac{2}{8}) + (\\frac{1}{8}) \\log_2 (\\frac{1}{8}) + (\\frac{2}{8}) \\log_2 (\\frac{2}{8}) $$<br>\n",
    "$$ ENTROPY = 0.571 $$\n",
    "\n",
    "- We will calculate the entropy of the dataset similarly after every split to calculate the gain\n",
    "- Gain can be calculated by finding the difference of the subsequent entropy values after split\n",
    "- Now we will try to choose a condition that gives us the highest gain\n",
    "- We will do that by splitting the data using each condition and checking the gain that we get out them\n",
    "- Here are possible conditions:\n",
    "\n",
    "<img src=\"Pictures/85.png\">\n",
    "    \n",
    "- We will split the data by the color yelow:\n",
    "\n",
    "<img src=\"Pictures/86.png\">\n",
    "    \n",
    "- The entropy after splitting has decreased considerably\n",
    "- However, we still need some splitting at both the branches to attain an entropy value equal to zero\n",
    "- So, we decide to split both the nodes using **height** as condition:\n",
    "\n",
    "<img src=\"Pictures/87.png\">\n",
    "    \n",
    "- Since every branch now contains single label type, we can say that the entropy in this case has reached the least value\n",
    "- This tree can now predict all the classes of animals present in the dataset with 100% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.7. Random Forest Algorithm<a class=\"anchor\" id=\"1.4.7.\"></a>\n",
    "\n",
    "- Used in ETM devices to acquire image of the earth's surface\n",
    "- Multiclass object detection is done using Random FOrest algorithms\n",
    "- Random forest is used in a game console called Kinect - tracks body movement and recreates it in game\n",
    "    - Training set to identify body parts\n",
    "    - Goes to random forest classifier\n",
    "    - Identifies the body parts while dancing\n",
    "    - Makes score\n",
    "    \n",
    "    \n",
    "### Why Random Forest?\n",
    "- No overfitting\n",
    "    - use of multiple trees reduce the risk of overfitting\n",
    "    - training time is less\n",
    "- High accuracy\n",
    "    - runs efficiently on large database\n",
    "    - for large data, it produces highly accurate predictions\n",
    "- Estimates missing data\n",
    "    - Random Forest can maintain accuracy when a large propotrion of data is missing\n",
    "    \n",
    "    \n",
    "### What is Random Forest?\n",
    "- Random forest or Random Decision Forest is a method that operates by constructing multiple Decision Trees during training phase\n",
    "- The decision of the majority of the trees is chosen by the random forst as the final decision\n",
    "\n",
    "<img src=\"Pictures/88.png\">\n",
    "   \n",
    "\n",
    "#### Decision Tree\n",
    "- Tree shaped diagram used to determine a course of action\n",
    "- Each branch of the tree represents a possible decision, occurrence or reaction\n",
    "\n",
    "<img src=\"Pictures/89.png\">\n",
    "\n",
    "    \n",
    "**ENTROPY**\n",
    "- The measure of randomness or unpredictability in the dataset\n",
    "- Entropy is high when the data isn't split yet\n",
    "- Get's lower as data is split into groups\n",
    "\n",
    "<img src=\"Pictures/90.png\">\n",
    "\n",
    "\n",
    "**INFORMATION GAIN**\n",
    "- Measure of decrease in entropy after the dataset is split\n",
    "- For the above example\n",
    "    - We've gone from 1 set with high entropy to 2 sets with lower entropy\n",
    "    - Information gain is simply Entropy 2 - Entropy 1\n",
    "    \n",
    "\n",
    "**LEAF NODE**\n",
    "- Carries the classification or the decision - final decision\n",
    "\n",
    "\n",
    "**DECISION NODE**\n",
    "- Decision node has two or more branches\n",
    "\n",
    "<img src=\"Pictures/91.png\">\n",
    "\n",
    "**ROOT NODE**\n",
    "- The top most decision node\n",
    "\n",
    "<img src=\"Pictures/92.png\">\n",
    "    \n",
    "    \n",
    "#### How does a Decision Tree work?\n",
    "- We have a ball of fruit\n",
    "- Problem statement: *Classify the different types of fruits in the bowl based on different features*\n",
    "\n",
    "<img src=\"Pictures/93.png\">\n",
    "\n",
    "- The dataset is looking quite messy and the entropy is high in this case\n",
    "- We'll start with the training set\n",
    "\n",
    "<img src=\"Pictures/94.png\">\n",
    "\n",
    "- We have to frame the conditions that split the data in such way that the **information gain is the highest**\n",
    "    - gain is the measure of decrease in entropy after splitting\n",
    "- Now we will try to choose a condition that gives us the highest gain\n",
    "- We will do that by splitting the data using each condition and checking the gain that we get out of them\n",
    "- Here are those different conditions:\n",
    "\n",
    "<img src=\"Pictures/95.png\"> \n",
    "\n",
    "- As we have 4 fruits with the diameter of 3, we can say that it is the best condition for the data split\n",
    "\n",
    "<img src=\"Pictures/96.png\">\n",
    "\n",
    "- The entropy after splitting has decreased considerably\n",
    "- The left node has already attained an entropy value of zero because there is only one kind of label in this branch\n",
    "    - No further splitting required for it\n",
    "- However, node on the right still requires a split to decrease the entropy further\n",
    "    - So we split it by color\n",
    "    \n",
    "<img src=\"Pictures/97.png\">\n",
    "    \n",
    "- The entropy in this case is zero - we can make predictions with 100% accuracy!\n",
    "\n",
    "\n",
    "### How does Random Forest work?\n",
    "- Let this be tree 1:\n",
    "\n",
    "<img src=\"Pictures/98.png\">\n",
    "    \n",
    "- Let this be tree 2:\n",
    "\n",
    "<img src=\"Pictures/99.png\">\n",
    "    \n",
    "- And let this be tree 3:\n",
    "\n",
    "<img src=\"Pictures/100.png\">\n",
    "    \n",
    "- Now let's try to classify this fruit:\n",
    "\n",
    "<img src=\"Pictures/101.png\">\n",
    "    \n",
    "- It's blacked because one of the Random Forest strengths is that it works good with missing data\n",
    "- TREE 1 classifies it as an orange\n",
    "    - Diameter = 3\n",
    "    - Color = Orange\n",
    "    - Grows in summer = yes\n",
    "    - Shape = circle\n",
    "- TREE 2 classifies it as cherries\n",
    "    - Diameter = 3\n",
    "    - Color = Orange\n",
    "    - Grows in summer = yes\n",
    "    - Shape = circle\n",
    "- TREE 3 classifies it as orange\n",
    "    - Diameter = 3\n",
    "    - Color = Orange\n",
    "    - Grows in summer = yes\n",
    "    - Shape = circle\n",
    "    \n",
    "- 2 oranges, 1 cherry, majority of votes says orange - it is classified as an orange!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.8. K Means Clustering <a class=\"anchor\" id=\"1.4.8.\"></a>\n",
    "\n",
    "\n",
    "- Unsupervised ML algorithm, performs divison of objects into clusters which are similar between them and are dissimilar to the objects belonging to another cluster\n",
    "- Let's take an example of Cricket\n",
    "    - The data contains runs and wickets in the last 10 matches\n",
    "    - So, the bowler will have more wickets and the batsmen will have higher runs\n",
    "    \n",
    "\n",
    "<img src=\"Pictures/127.png\">\n",
    "\n",
    "\n",
    "- Initially, two centroids are assigned randomly\n",
    "- The Euclidean distance is used to find out which centroid is closest to each data point and the data points are assigned to the corresponding centroids\n",
    "\n",
    "\n",
    "<img src=\"Pictures/128.png\">\n",
    "\n",
    "- The next step is to reposition the centroids for optimization\n",
    "\n",
    "\n",
    "<img src=\"Pictures/129.png\">\n",
    "\n",
    "- The process is continued until the centroids become static:\n",
    "\n",
    "\n",
    "<img src=\"Pictures/130.png\">\n",
    "\n",
    "\n",
    "### Types of Clustering\n",
    "\n",
    "\n",
    "<img src=\"Pictures/131.png\">\n",
    "\n",
    "- Hierarchial Clustering\n",
    "    - Clusters have a tree like structure\n",
    "    - Divided into:\n",
    "        - Agglomerative\n",
    "            - Bottom up approach: Behin with each element as a separate cluster and merge them into successively large clusters\n",
    "        - Divisive\n",
    "            - Top Down approach behin with the whole set and proceed to divide it into successively smaller clusters\n",
    "- Partitional Clustering\n",
    "    - Divided into:\n",
    "        - K-Means\n",
    "            - Division of objects into clusters such that each object is in exactly one cluster, not several\n",
    "        - Futty C-Means\n",
    "            - Divison of objects into clusters such that each object can belong to multiple clusters\n",
    "            \n",
    "            \n",
    "\n",
    "### Applications of K-Means Clustering\n",
    "- Academic Performance\n",
    "- Diagnosic Systems\n",
    "- Search Engine\n",
    "- Wireless Sensor Network's\n",
    "\n",
    "\n",
    "### Distance Measure\n",
    "- Distance measure will determine the similarity between two elements and it will influence the shape of the clusters\n",
    "- Differen measure types:\n",
    "    - Euclidean distance measure\n",
    "    - Manhattan distance measure\n",
    "    - Squared Euclidean distance measure\n",
    "    - Cosine distance measure\n",
    "    \n",
    "    \n",
    "#### Euclidean Distance Measure\n",
    "- Ordinary straight line\n",
    "- Distance between two points in Euclidean space\n",
    "\n",
    "$$ \\large d = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2} $$\n",
    "\n",
    "\n",
    "<img src=\"Pictures/132.png\">\n",
    "\n",
    "\n",
    "#### Squared Euclidean Distance Measure\n",
    "- The Euclidean squared distance metric uses the same equation as the Euclidean distance metric, but does not take the square root\n",
    "\n",
    "$$ \\large d = \\sum_{i=1}^{n} (q_i - p_i)^2 $$\n",
    "\n",
    "\n",
    "#### Manhattan Distance Measure\n",
    "- The Manhattan distance is the simple sum of the horizontal and vertical components or the distance between two points measured along axes at right angles\n",
    "\n",
    "$$ \\large d = \\sum_{i=1}^{n} | q_x - p_x | + | q_y - p_y | $$\n",
    "\n",
    "\n",
    "<img src=\"Pictures/133.png\">\n",
    "\n",
    "#### Cosine Distance Measure\n",
    "- The cosine distance similarity measures the angle between the two vectors\n",
    "\n",
    "$$ \\large d = \\frac{\\sum_{i=0}^{n-1} q_i - p_x}{\\sum_{i=0}^{n-1} (q_i)^2 * \\sum_{i=0}^{n-1} (p_i)^2} $$\n",
    "\n",
    "\n",
    "<img src=\"Pictures/134.png\">\n",
    "\n",
    "\n",
    "### How Does K-Means Clustering Work\n",
    "\n",
    "\n",
    "<img src=\"Pictures/135.png\">\n",
    "\n",
    "\n",
    "#### Elbow Point\n",
    "- Let's say, you have a dataset for a Grocery Shop\n",
    "\n",
    "\n",
    "<img src=\"Pictures/136.png\">\n",
    "\n",
    "- Now the important question is, **\"How would you choose the optimum number of clusters?\"**\n",
    "- The best way to do this is by **Elbow Method**\n",
    "- The idea of the elbow method is to run K-Means clustering on the dataset where \"k\" is reffered as number of clusters\n",
    "- Within sum of squared (WSS) is defined as the sum of the squared distance between each member of the cluster and its centroid\n",
    "\n",
    "$$ \\large WSS = \\sum_{i=1}^{m} (x_i - c_i)^2 $$\n",
    "\n",
    "- Where $ x_i $ is the data point and $ c_i $ is the closest point to the centroid\n",
    "\n",
    "\n",
    "<img src=\"Pictures/137.png\">\n",
    "\n",
    "- Now, we draw a curve between WSS and the number of clusters\n",
    "- Here, we can see a very slow change in the value of WSS after k=2, so you should take that elbow point value as the final number of clusters\n",
    "\n",
    "\n",
    "#### Measure the Distance\n",
    "1. The given data points below are assumed as **delivery points**\n",
    "\n",
    "\n",
    "<img src=\"Pictures/138.png\">\n",
    "\n",
    "2. We can randomly initialize two points called the cluster centroid, **Euclidean distance** is a distance measure used to find out which data poin is closest to our centroids\n",
    "\n",
    "\n",
    "<img src=\"Pictures/139.png\">\n",
    "\n",
    "\n",
    "#### Grouping \n",
    "3. Based upon the distance from c1 and c2 centroids, the data points will group itself into clusters\n",
    "\n",
    "\n",
    "<img src=\"Pictures/140.png\">\n",
    "\n",
    "#### Reposition the centroids\n",
    "4. Compute the centroid of data point inside the blue cluster\n",
    "5. Reposition the centroid of the blue cluster to the new centroid\n",
    "\n",
    "\n",
    "<img src=\"Pictures/141.png\">\n",
    "\n",
    "6. Now, compute the centroid of data points inside the orange cluster\n",
    "7. Reposition the centroid of the orange cluster to the new centroid\n",
    "\n",
    "\n",
    "<img src=\"Pictures/142.png\">\n",
    "\n",
    "\n",
    "#### Convergence\n",
    "8. Once the clusters become static, K-Means clustering algorithm is said to be converged\n",
    "\n",
    "\n",
    "<img src=\"Pictures/143.png\">\n",
    "\n",
    "\n",
    "\n",
    "### K-Means Clustering Algorithm\n",
    "Assuming we have inputs **x1, x2, x3, ....** and a value of *k*\n",
    "1. Pick k random points as cluster centers called centroids\n",
    "2. Assign each **xi** to the nearest cluster by calculating its distance to each centroid\n",
    "3. Find new cluster center by taking the average of the assigned points\n",
    "4. Repeat Step 2 and 3 until none of the cluster assignents change\n",
    "\n",
    "\n",
    "**DETAILED LOOK AT EACH OF THOSE STEPS**\n",
    "**Step 1:**\n",
    "\n",
    "We randomly pick **K** cluster centers (centroids). Let's assume these are c1, c2, ... ckc1, c2, ... ck, and we can say that:\n",
    "\n",
    "$$ \\large C = C_1, C_2, ... C_k $$\n",
    "\n",
    "C is the set of all centroids.\n",
    "\n",
    "\n",
    "**Step 2:**\n",
    "\n",
    "In this step, we assign each data point to closest center, this is done by calculating Euclidean distance:\n",
    "\n",
    "$$ \\large arg min, dist(c_i, x)^2 | c_i \\in C $$\n",
    "\n",
    "Where **dist()** is the Euclidean distance.\n",
    "\n",
    "\n",
    "**Step 3:**\n",
    "\n",
    "In this step, we find the new centroid by taking the average of all the points assigned to that cluster:\n",
    "\n",
    "$$ \\large c_i = \\frac{1}{| S_i |} \\sum x_i  | x_i \\in S_i $$\n",
    "\n",
    "$ S_i $ is the set of all points assigned to the i th cluster\n",
    "\n",
    "\n",
    "**Step 4:**\n",
    "\n",
    "In this step, we repeat step 2 and 3 until none of the cluster assignments change. That means until out cluster remain stable, we repeat the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4.9. Logistic regression <a class=\"anchor\" id=\"1.4.9.\"></a>\n",
    "\n",
    "- Logistic Regression is one of the algorithms used for classification\n",
    "- Imagine it's been a few years since you serviced your car\n",
    "    - How long until card breaks down?\n",
    "    - Here's the graph showing probabilities of car breaking down after x years of service\n",
    "\n",
    "- The logistic regression algorith is the simplest classification algorith used for binary or multi-classification problems\n",
    "- To brush up,\n",
    "\n",
    "$$ \\large y = mx + c $$\n",
    "\n",
    "\n",
    "- A classification algorithm is used to predict binary outcomes for a given set of independent variables. The dependent variable's outcome is discrete\n",
    "- The dependent variable is the target class variable we are going to predict\n",
    "- The independent variables are the features or attributes we are going to use to predict the target class\n",
    "\n",
    "<img src=\"Pictures/43.png\">\n",
    "\n",
    "- We know what a linear regression looks like, but using this graph we cannot divide the outcome into categories\n",
    "\n",
    "### Linear Regression VS. Logistic Regression\n",
    "- **Linear regression** \n",
    "    - the resposne variables are continuous in nature ( salay of 400 dollars , 500 dollars , 745 dollars)\n",
    "    - Used to solve **regression** problems\n",
    "    - the resposne variables are continuous in nature\n",
    "    - it helps to estimate the dependent variable when there is a change in the independent variable\n",
    "    - it is a traight line\n",
    "    \n",
    "- **Logistic regression** \n",
    "    - the response variable is categorical(discreate) in valuehas (YES/NO, Cat or a dog)\n",
    "    - Used to solve **classification** problems\n",
    "    - it helps to calculate the possibility of a particular event taking place\n",
    "    - An S-curve (S = sigmoid)\n",
    "\n",
    "- Here are two graphs, showing employee rating on the x-axis and whether or not the employee got a promotion on the y-axis:\n",
    "    - \n",
    "\n",
    "<img src=\"Pictures/73.png\">\n",
    "    \n",
    "- As we can see, the error value when using this type of data with linear regression would be huge\n",
    "- This is because the employee is either promoted or isn't, there's nothing in between\n",
    "- For this cases, when the dependent variable is categorical, we use logistic regression\n",
    "\n",
    "\n",
    "### The Math behing Logistic Regression\n",
    "\n",
    "- To understand Logistic Regression, let's talk about the odds of success\n",
    "- We can get concrete probability of something happening if we divide the probability of event happening with probability of event not happening\n",
    "- Mathematically:\n",
    "\n",
    "$$ \\large Odds (\\theta) = \\frac{p}{1 - p} $$\n",
    "\n",
    "- The value of odds range from 0 to $ \\infty $\n",
    "- The values of probability change from 0 to 1\n",
    "- Remember the equation of the straight line\n",
    "    - $ \\beta_0 $ is the y-intercept\n",
    "    - $ \\beta_1 $ is the slope of the line\n",
    "    - $ x $ is the valur of the x coordinate\n",
    "    - $ y $ is the value of the prediction\n",
    "    - EQUATION: $ y = \\beta_0 + \\beta_1 x $\n",
    "    \n",
    "<img src=\"Pictures/75.png\">\n",
    "\n",
    "- If we take the equation for the odds of event happening and combine it with the straight line equation:\n",
    "\n",
    "$$ \\log(\\frac{p(x)}{1 - p(x)}) = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "Exponentiating both sides gets to:\n",
    "\n",
    "$$ e^{ln}(\\frac{p(x)}{1-p(x)}) = e^{\\beta_0 + \\beta_1 x} $$<br>\n",
    "$$ (\\frac{p(x)}{1-p(x)}) = e^{\\beta_0 + \\beta_1 x} $$\n",
    "\n",
    "Let Y = $ e^{\\beta_0 + \\beta_1 x} $\n",
    "\n",
    "Then $ \\frac{p(x)}{1 - p(x)} = Y $\n",
    "\n",
    "$$ p(x) = Y(1 - p(x)) $$<br>\n",
    "$$ p(x) = Y - Y(p(x)) $$<br>\n",
    "$$ p(x) + Y(p(x)) = Y $$<br>\n",
    "$$ p(x)(1 + Y) = Y $$<br>\n",
    "$$ p(x) = \\frac{Y}{1 + Y} $$<br>\n",
    "$$ p(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} $$<br>\n",
    "\n",
    "The equation of a sigmoid function:\n",
    "\n",
    "$$ p(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}} $$<br>\n",
    "$$ p(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}} $$<br>\n",
    "\n",
    "- When this is plotted, a sigmoid curve is obtained:\n",
    "\n",
    "<img src=\"Pictures/74.png\">\n",
    "\n",
    "### Expmple 1. Hours of studies needed to pass the exam\n",
    "\n",
    "- For example, a linear regression graph can tell us that with increase in number of hours studied, the marks of a student will increase\n",
    "- But it will not tell us whether the student will pass or not\n",
    "- In such cases, where we need the output as **categorical value**, we will use logistic regression\n",
    "- For that we will use the **Sigmoid function**:\n",
    "\n",
    "<img src=\"Pictures/44.png\">\n",
    "\n",
    "- We can zoom in on the function:\n",
    "\n",
    "<img src=\"Pictures/45.png\">\n",
    "\n",
    "### Expmple 2. Car breakdown\n",
    "\n",
    "- Imagine it's been a few years since you serviced your car\n",
    "    - How long until card breaks down?\n",
    "    - Here's the graph showing probabilities of car breaking down after x years of service\n",
    "    \n",
    "<img src=\"Pictures/71.png\">\n",
    "\n",
    "- Probabiltiy goes from 0 to 1\n",
    "- The fewer the years since the last service, the lesser the probability of car breakdown\n",
    "- Here's a visualization of how Logistic Regression would work:\n",
    "\n",
    "<img src=\"Pictures/72.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 2. Clustering <a class=\"anchor\" id=\"2.\"></a>\n",
    "\n",
    "-  **Clustering** = organizing object into groups based on their similarity\n",
    "- clustering with known categories or exploring data to find out categories \n",
    "\n",
    "## K-means Clustering\n",
    "\n",
    "- example of unsupervised learning\n",
    "- it is used when you have unlabeled data\n",
    "- to find clusters in data based of feature similarity\n",
    "- **Converging** = what ever math we are using to figure out the answer has come to solution or to converge on a answer\n",
    "- **Elbow Method**\n",
    "    - Finding the optimal number of clusters using the elbow of the graph\n",
    "    - **WSS** - Within sum of squares (WSS) is defined as the sum of the squared distance between each member of the cluster and its centorid\n",
    "    \n",
    "<img src=\"Pictures/34.png\">\n",
    "\n",
    "\n",
    "**steps of K-means Clustering:**\n",
    "\n",
    "<img src=\"Pictures/30.png\">\n",
    "**1. Initialize Cluster Centroids**\n",
    "    - We pick \"K\" clusters and assign random centroids to clusters\n",
    "    -  Choosing the right value of K will help in less number of iterations\n",
    "    \n",
    "<img src=\"Pictures/31.png\">\n",
    "\n",
    "**2. Compute minimum distance**\n",
    "    - Comupute distancd from objects to centorids\n",
    "    - form new clusters based on minimum distance and calculate their centroids\n",
    "    \n",
    "<img src=\"Pictures/32.png\">\n",
    "\n",
    "**3. Assign points to new cluster**\n",
    "    - repeate previous 2 steps iteratively till the cluster centroids stop changing their positions and become static\n",
    "    - when the clusters become static then k-means clustering algorithm is said to be converged\n",
    "    \n",
    "\n",
    "## Flowchart to understand K-Means  \n",
    "\n",
    "<img src=\"Pictures/33.png\">\n",
    "    \n",
    "### Example\n",
    "- Suppose we have this dataset of 7 individuals and their score on two:\n",
    "\n",
    "<img src=\"Pictures/35.png\">\n",
    "\n",
    "- Now let's take two farthest-apart points as initial cluster centroids\n",
    "    - Subject 1 and 4, both points\n",
    "    \n",
    "<img src=\"Pictures/36.png\">\n",
    "\n",
    "- Red circles represent the farthest-apart points and each point is then assigned to the closest cluster with respect to their distance from the centroids\n",
    "- We again calculate the centroids of each cluster:\n",
    "\n",
    "<img src=\"Pictures/37.png\">\n",
    "\n",
    "- We compare each individual's distance to its own cluster mean and to that of the opposite cluster\n",
    "- We find:\n",
    "\n",
    "<img src=\"Pictures/38.png\">\n",
    "\n",
    "- Only the individual 3 is nearer to the mean of the opposite cluster (Cluster2) than its own (Cluster 1)\n",
    "\n",
    "<img src=\"Pictures/39.png\">\n",
    "\n",
    "- Thus, individual 3 is relocated to Cluster 2 resulting in the new partition:\n",
    "\n",
    "<img src=\"Pictures/40.png\">\n",
    "\n",
    "- For the new clusters, we will find the actual cluster centroids:\n",
    "\n",
    "<img src=\"Pictures/41.png\">\n",
    "\n",
    "- On comparing the distance of each individual's distance to it's own cluster mean and to that of the opposite cluster, we find that the data points are stable, hence we find that the data points are stable, hence we have out final clusters\n",
    "\n",
    "#### Choosing the right K value\n",
    "- Choosing the right value of k will help in less number of iterations\n",
    "- To find appropriate number of clusters in a dataset, we use elbow method\n",
    "    - Finding the optimal number of clusters using the elbow of the graph\n",
    "- WSS - within sum of squares - sum of the squared distance between each member of the cluster and it's centroid\n",
    "\n",
    "<img src=\"Pictures/42.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
