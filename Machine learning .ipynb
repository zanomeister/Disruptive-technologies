{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3 align=\"center\"> <font size=\"6\"> Machine learning notebook </h3>\n",
    "    \n",
    " **Table of content:**\n",
    "* [1. Machine learning](#1.)\n",
    "    * [1.2. Types of machine learning](#1.2.)\n",
    "    * [1.3. Machine Learning vs Deep Learning vs AI](#1.3.)\n",
    "    * [1.4. Machine learning algorithms](#1.4.)\n",
    "        * [1.4.1. Linear regression](#1.4.1.)\n",
    "        * [1.4.2. Decision Trees](#1.4.2.)\n",
    "        * [1.4.3. Support vector machine](#1.4.3.)\n",
    "* [2. Clustering](#2.)\n",
    "* [3. Logistic Regression](#3.)\n",
    "\n",
    "\n",
    "# 1. Machine learning <a class=\"anchor\" id=\"1.\"></a>\n",
    "\n",
    "## What is machine learning?\n",
    "\n",
    "Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead. \n",
    "\n",
    "In other words, ML is a techique which uses statistical method, enabling machines to learn from their past data, predicts whats going to come next and improves its self.\n",
    "\n",
    "Also, ML is the science of making computers learn and act like humans by feeding data and information without being explicitly programmed.\n",
    "\n",
    "ML is a subset of artificial inteligence:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/8.png \"ShowMyImage\")\n",
    "<img align=\"center\" src=\"/notebooks/ML/Pictures/8.png\" width=500 > ![](ML/8.png)\n",
    "![](ML/Pictures/8.png)\n",
    "\n",
    "Steps of ML:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/9.png \"ShowMyImage\")\n",
    "\n",
    "## Machine Learning\n",
    "- The system is able to make predictions or take decisions based on past data\n",
    "    - Better decisions and predictions\n",
    "    - Quick and accurate outcomes\n",
    "    - More powerful processing capablity\n",
    "    - Analyzing complex big data\n",
    "    - Managing large amounts of data\n",
    "    - Inexpensive\n",
    "\n",
    "**Basic divisions what ML does:**\n",
    "- Classification\n",
    "    - predicting of a category (yes/no, 0/1)\n",
    "    - example : will the stock price increase or decrease\n",
    "- Regression\n",
    "    - prediction of a quantity\n",
    "    - predicting the age of a person based on the height, health and other factors\n",
    "- Anomaly detection\n",
    "    - detection of a anomaly\n",
    "    - detect money withdrawal anomalies / stock exchage anomaly\n",
    "- Clustering\n",
    "    - discovering structure in unexplored/unknown data\n",
    "    - finding groups of customers with similar behavior given a large database of customer data containing their demographics and past buying records\n",
    "        \n",
    "        \n",
    "\n",
    "## Why machine learning?\n",
    "- autonomous cars\n",
    "- disease detection\n",
    "- voice and face recognition\n",
    "- spam detection /engagement bait \n",
    "\n",
    "\n",
    "## Visualisation of machine learning process:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/1.png \"ShowMyImage\")\n",
    "\n",
    "# 1.2. Types of machine learning <a class=\"anchor\" id=\"1.2.\"></a>\n",
    "\n",
    "1. Supervised learning\n",
    "2. Unsupervised learning\n",
    "3. Reinforcement learning\n",
    "\n",
    "**1. Supervised learning**\n",
    "\n",
    "- Uses labeled data to teach the model\n",
    "- we give machine samples so that machine can learn from it\n",
    "- Classification and regression are the two most common types of supervised learning\n",
    "- Systems are able to predict future outcomes based on past data\n",
    "- Requires both an input and output to be given to the model for it to be trained\n",
    "\n",
    "**Most popular algorithms:**\n",
    "\n",
    "![title](/notebooks/ML/Pictures/3.png \"ShowMyImage\")\n",
    "\n",
    "**Supervised learning visualisation:**\n",
    "![title](/notebooks/ML/Pictures/2.png \"ShowMyImage\")\n",
    "\n",
    "![title](/notebooks/ML/Pictures/10.png \"ShowMyImage\")\n",
    "\n",
    "**2. Unsupervised learning**\n",
    "\n",
    "- Learning with unlabeled dana\n",
    "- No predetermined result in advance. Learning through exploration\n",
    "- Clustering and Association are the two most common types of unsupervised learning\n",
    "- Systems are able to identify hidden patterns from the input data provided (unlabeled data)\n",
    "- By making the data more readable and organized, the patterns, similarities, or anomalies become more evident\n",
    "\n",
    "**Most popular algorithms:**\n",
    "![title](/notebooks/ML/Pictures/5.png \"ShowMyImage\")\n",
    "\n",
    "**Unsupervised learning visualisation:**\n",
    "![title](/notebooks/ML/Pictures/4.png \"ShowMyImage\")\n",
    "\n",
    "**3. Reinforcement Learning**\n",
    "- Systems are given no training\n",
    "- It learns on the basis of the reward/punishment it recieved for performing its last action\n",
    "- learning how to learn\n",
    "- agent learns how to behave in an enviroment by performing actions and seeing the results\n",
    "- It helps increase the efficiency of a tool/function or a program\n",
    "- similar learning pattern as a humas (trial and error)\n",
    "    \n",
    "**Reinforcement learning visualisation:**\n",
    "![title](/notebooks/ML/Pictures/11.png \"ShowMyImage\")\n",
    "\n",
    "## Supervised VS unsupervised ML\n",
    "\n",
    "- can be  combined - you have image and not sure what you're looking for(unstructured data), first find all things connected together, then somebody labeles that data. Now you can use labeled data to predict\n",
    "\n",
    "![title](/notebooks/ML/Pictures/12.png \"ShowMyImage\")\n",
    "\n",
    "## Some Applications of ML\n",
    "- Instance Segmentation\n",
    "- Number Plate Detection\n",
    "- Automatic Translation\n",
    "- ...\n",
    "\n",
    "\n",
    "# 1.3. Machine Learning vs Deep Learning vs AI <a class=\"anchor\" id=\"1.3.\"></a>\n",
    "\n",
    "\n",
    "## Humans vs AI\n",
    "- **Human Intelligence**\n",
    "    - Use available information to take decisions\n",
    "    - Communicate with people\n",
    "    - Identify patterns in data\n",
    "    - Remember what people have said\n",
    "    - Adapt to new situations\n",
    "- **Artificial Intelligence**\n",
    "    - AI develops computer systems that can accomplish tasks that require human inteligence\n",
    "    - Interact with humans using their natural language\n",
    "    - Provides more accurate results\n",
    "    - Learns from their mistakes and adapt to new environments\n",
    "    - Learns from the data and automates repetitive learning\n",
    "    \n",
    "## Deep Learning\n",
    "- Systems think and learn like humans using artificial neaural networks\n",
    "    - Performance improves with more data\n",
    "    - Better scalability\n",
    "    - Problem solved in an end-to-end method\n",
    "    - Best features are selected by the system\n",
    "    - Is a subset of ML\n",
    "    - Lesser testing time\n",
    "    \n",
    "## Real-life Examples\n",
    "**Artificial Intelligence**\n",
    "- News generation\n",
    "- Smart Home Devices (e.g. Amazon Echo)\n",
    "\n",
    "**Machine Learning**\n",
    "- Spam detection\n",
    "- Search engine result refining\n",
    "\n",
    "**Deep Learning**\n",
    "- Automatic translation\n",
    "- Chatbots\n",
    "\n",
    "\n",
    "## Types of AI\n",
    "**1. Reactive Machines**\n",
    "- Systems that only react\n",
    "- Don't form memories\n",
    "- Doesn't use any past experiences for new decisions\n",
    "    \n",
    "**2. Limited Memory**\n",
    "- Systems look into the past\n",
    "- Information is added over a period of time\n",
    "- Information is short lived\n",
    "    \n",
    "**3. Theory of Mind**\n",
    "- Systems being able to understand human emotions and how they effect decision making\n",
    "- To adjust their behavious according to their human understanding\n",
    "    \n",
    "**4. Self-awareness**\n",
    "- Systems being aware of themselves\n",
    "- Understand their own internal states\n",
    "- Predicting other people's geeling and act appropriately\n",
    "    \n",
    "    \n",
    "\n",
    "## Comparing ML and DL\n",
    "**Machine Learning**\n",
    "- Enables machines to take decisions on their own, based on the past data\n",
    "- Needs only a small amount of training data\n",
    "- Works well on low-end systems\n",
    "- Most features need to be identified in advance and manually coded\n",
    "- The problem is divided into parts and solved individualy and then combined\n",
    "- Testing takes longer\n",
    "- Crisp rules explain why a certain decision was taken\n",
    "\n",
    "**Deep Learning**\n",
    "- Enables machines to take decisions with the help of artificial neural networks\n",
    "- Needs a large amount of training data\n",
    "- Needs high end systems to work\n",
    "- The machine learns the features from the data it is provided \n",
    "- The problem is solved in an end-to-end manner\n",
    "- Testing takes less time\n",
    "- Since the system takes decisions based on it's own logic, the reasoning may be difficult to interpret\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. Machine learning algorithms <a class=\"anchor\" id=\"1.4.\"></a>\n",
    "\n",
    "## 1.4.1. Linear regression <a class=\"anchor\" id=\"1.4.1.\"></a>\n",
    "- linear regression is one of the most well known and well understood algorithms in statistics and machine learning\n",
    "- linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and a single output variable (y)\n",
    "![title](/notebooks/ML/Pictures/16.png \"ShowMyImage\")\n",
    "- mimimizing the distance \n",
    "    - moving line trough the data points to make sure the best fit line has least square distance between the data points and the regression line\n",
    "    - there are lots of ways to minimize the distance between the line and the data points like **Sum of squared errors, Sum of absoulute errors, Root mean square error etc.**\n",
    "\n",
    "## Example 1. Variables having positive and negative relationships**\n",
    "\n",
    "![title](/notebooks/ML/Pictures/13.png \"ShowMyImage\")\n",
    "\n",
    "![title](/notebooks/ML/Pictures/14.png \"ShowMyImage\")\n",
    "\n",
    "![title](/notebooks/ML/Pictures/15.png \"ShowMyImage\")\n",
    "\n",
    "## Example 2. Mathematical implementation of Linear regression**\n",
    "\n",
    "![title](/notebooks/ML/Pictures/17.png \"ShowMyImage\")\n",
    "**Xi and Yi are the mean values of X and Y. The line goes through that point. Now it's time to calculate m.**\n",
    "![title](/notebooks/ML/Pictures/20.png \"ShowMyImage\")\n",
    "\n",
    "**Now we can calculate the value of c. Point c is the staeting point of the line on y axis**\n",
    "![title](/notebooks/ML/Pictures/22.png \"ShowMyImage\")\n",
    "![title](/notebooks/ML/Pictures/21.png \"ShowMyImage\")\n",
    "\n",
    "**Predicting the values of y using x =(1,2,3,4,5) and ploting the points**\n",
    "- Calculating what Y thinks they are, not what they actualy are. \n",
    "- Yp = Predicted values of Y (of what we think its going to be when we plug those numbers in the plot)\n",
    "![title](/notebooks/ML/Pictures/23.png \"ShowMyImage\")\n",
    "![title](/notebooks/ML/Pictures/24.png \"ShowMyImage\")\n",
    "\n",
    "## 1.4.2. Decision Trees <a class=\"anchor\" id=\"1.4.2.\"></a>\n",
    "\n",
    "- Decision Tree is a tree shaped algorithm used to determine a course of action. \n",
    "- Each branch of the tree represents a possible decision, occurrence or reaction\n",
    "\n",
    "\n",
    "\n",
    "# Example 1. Determining wheter to play or not to play golf\n",
    "\n",
    "![title](/notebooks/ML/Pictures/46.png \"ShowMyImage\")\n",
    "\n",
    "- our data are : \n",
    "    - Outlook (Rainy, Sunny, Overcast)\n",
    "    - Temperature (hot, mild, cool)\n",
    "    - Humidity (high, normal)\n",
    "    - Windy (fals, true)\n",
    "\n",
    "![title](/notebooks/ML/Pictures/25.png \"ShowMyImage\")\n",
    "\n",
    "**How to decide how to split the data up? Is this the right decision tree?**\n",
    "For that we need to calculate Entropy and information gain\n",
    "\n",
    "![title](/notebooks/ML/Pictures/26.png \"ShowMyImage\")\n",
    "\n",
    "**Entropy**\n",
    "- is the measure od randomness or \"impurity\" in the dataset\n",
    "- the lower number the better\n",
    "- we can calculate Entropy of target class of the data set (whole entropy). Target class of the data set is should we play golf or not\n",
    "- we can also calculate the entropy for playing gold and the outlook. \n",
    "\n",
    "$$ Entropy = I(p, n) = - \\frac{p}{p + n} * \\log_{2} (\\frac{p}{p + n}) - \\frac{n}{p + n} * \\log_{2} (\\frac{n}{p + n}) $$\n",
    "\n",
    "$ p $ = probability that we would play the game of golf<br>\n",
    "$ n $ = probability that we would not play the game of golf<br>\n",
    "\n",
    "- In the above dataset there are 14 data points\n",
    "    - 5 state that we shouldn't play golf\n",
    "    - 9 state that we should\n",
    "- So it's easy to calculate entropy:\n",
    "\n",
    "$$ \\large E(5, 9) $$<br>\n",
    "$$ \\large I(5/14, 9/14) $$<br>\n",
    "$$ \\large I(0.36, 0.64) $$<br>\n",
    "$$ \\large = - (0.36 \\log_{2} 0.36) - (0.64 \\log_{2} 0.64) $$<br>\n",
    "$$ \\large = 0.94 $$<br>\n",
    "\n",
    "\n",
    "Similary, we can calculate the entropy of other predictors like Temperature, Humidity, Windy\n",
    "![title](/notebooks/ML/Pictures/27.png \"ShowMyImage\")\n",
    "\n",
    "**Information gain**\n",
    "- The greates number is the greatest gain of information and that is the split we want\n",
    "- \n",
    "![title](/notebooks/ML/Pictures/28.png \"ShowMyImage\")\n",
    "\n",
    "**Outlook is our greatest information gain and that is our fist split.** \n",
    "- Now we build the the new decision tree\n",
    "- Continue down the tree using the next greatest information gain \n",
    "![title](/notebooks/ML/Pictures/29.png \"ShowMyImage\")\n",
    "\n",
    "## 1.4.3. Support vector machine (SVM) <a class=\"anchor\" id=\"1.4.3.\"></a>\n",
    "\n",
    "- Support vector machine is a widely used classification algorithm\n",
    "- the idea of support vector machine is simple: The algorithm creates a separation line which divides the vlasses in the best possible manner\n",
    "    - for example = dog or car, disease or no disease\n",
    "- The goal of SVM is to choose a **hyperplane** with the greatest possible margin between the decision line and the nearest point within the training set\n",
    "    - **hyperplane** - is a multidimensional cut in the data\n",
    "- **distance margin =** The distance between the hyperplane and the nearest data point from either set\n",
    "\n",
    "- Suppose that we have labeled sample data, which tells height and weight of males and females:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/47.png \"ShowMyImage\")\n",
    "\n",
    "- A new data point arrives, and we want to know how can a machine classify whether a new data point is male or a female?\n",
    "\n",
    "![title](/notebooks/ML/Pictures/48.png \"ShowMyImage\")\n",
    "\n",
    "- The goal is to choose a hyperplane with the greatest possible margin betwwen the decision line and the nearest point within the training set\n",
    "    - **Distance margin** - distance between the hyperplane and the nearest data point from either set\n",
    "    \n",
    "![title](/notebooks/ML/Pictures/49.png \"ShowMyImage\")\n",
    "\n",
    "- When we draw the hyperplanes, we observe that Line 1 has the maximum distance margin so it will classify the new data point correctly\n",
    "- As the result, the new data point is **male**\n",
    "\n",
    "![title](/notebooks/ML/Pictures/50.png \"ShowMyImage\")>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 2. Clustering <a class=\"anchor\" id=\"2.\"></a>\n",
    "\n",
    "-  **Clustering** = organizing object into groups based on their similarity\n",
    "- clustering with known categories or exploring data to find out categories \n",
    "\n",
    "## K-means Clustering\n",
    "\n",
    "- example of unsupervised learning\n",
    "- it is used when you have unlabeled data\n",
    "- to find clusters in data based of feature similarity\n",
    "- **Converging** = what ever math we are using to figure out the answer has come to solution or to converge on a answer\n",
    "- **Elbow Method**\n",
    "    - Finding the optimal number of clusters using the elbow of the graph\n",
    "    - **WSS** - Within sum of squares (WSS) is defined as the sum of the squared distance between each member of the cluster and its centorid\n",
    "    \n",
    "![title](/notebooks/ML/Pictures/34.png \"ShowMyImage\")\n",
    "\n",
    "\n",
    "**steps of K-means Clustering:**\n",
    "\n",
    "![title](/notebooks/ML/Pictures/30.png \"ShowMyImage\")\n",
    "**1. Initialize Cluster Centroids**\n",
    "    - We pick \"K\" clusters and assign random centroids to clusters\n",
    "    -  Choosing the right value of K will help in less number of iterations\n",
    "    \n",
    "![title](/notebooks/ML/Pictures/31.png \"ShowMyImage\")\n",
    "\n",
    "**2. Compute minimum distance**\n",
    "    - Comupute distancd from objects to centorids\n",
    "    - form new clusters based on minimum distance and calculate their centroids\n",
    "    \n",
    "![title](/notebooks/ML/Pictures/32.png \"ShowMyImage\")\n",
    "\n",
    "**3. Assign points to new cluster**\n",
    "    - repeate previous 2 steps iteratively till the cluster centroids stop changing their positions and become static\n",
    "    - when the clusters become static then k-means clustering algorithm is said to be converged\n",
    "    \n",
    "\n",
    "## Flowchart to understand K-Means  \n",
    "\n",
    "![title](/notebooks/ML/Pictures/33.png \"ShowMyImage\")\n",
    "    \n",
    "### Example\n",
    "- Suppose we have this dataset of 7 individuals and their score on two:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/35.png)\n",
    "\n",
    "- Now let's take two farthest-apart points as initial cluster centroids\n",
    "    - Subject 1 and 4, both points\n",
    "    \n",
    "![title](/notebooks/ML/Pictures/36.png)\n",
    "\n",
    "- Red circles represent the farthest-apart points and each point is then assigned to the closest cluster with respect to their distance from the centroids\n",
    "- We again calculate the centroids of each cluster:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/37.png)\n",
    "\n",
    "- We compare each individual's distance to its own cluster mean and to that of the opposite cluster\n",
    "- We find:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/38.png)\n",
    "\n",
    "- Only the individual 3 is nearer to the mean of the opposite cluster (Cluster2) than its own (Cluster 1)\n",
    "\n",
    "![title](/notebooks/ML/Pictures/39.png)\n",
    "\n",
    "- Thus, individual 3 is relocated to Cluster 2 resulting in the new partition:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/40.png)\n",
    "\n",
    "- For the new clusters, we will find the actual cluster centroids:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/41.png)\n",
    "\n",
    "- On comparing the distance of each individual's distance to it's own cluster mean and to that of the opposite cluster, we find that the data points are stable, hence we find that the data points are stable, hence we have out final clusters\n",
    "\n",
    "#### Choosing the right K value\n",
    "- Choosing the right value of k will help in less number of iterations\n",
    "- To find appropriate number of clusters in a dataset, we use elbow method\n",
    "    - Finding the optimal number of clusters using the elbow of the graph\n",
    "- WSS - within sum of squares - sum of the squared distance between each member of the cluster and it's centroid\n",
    "\n",
    "![title](/notebooks/ML/Pictures/42.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic regression <a class=\"anchor\" id=\"3.\"></a>\n",
    "- The logistic regression algorith is the simplest classification algorith used for binary or multi-classification problems\n",
    "- To brush up,\n",
    "\n",
    "$$ \\large y = mx + c $$\n",
    "\n",
    "- The dependent variable is the target class variable we are going to predict\n",
    "- The independent variables are the features or attributes we are going to use to predict the target class\n",
    "\n",
    "![title](/notebooks/ML/Pictures/43.png)\n",
    "\n",
    "- We know what a linear regression looks like, but using this graph we cannot divide the outcome into categories\n",
    "- For example, a linear regression graph can tell us that with increase in number of hours studied, the marks of a student will increase\n",
    "- But it will not tell us whether the student will pass or not\n",
    "- In such cases, where we need the output as **categorical value**, we will use logistic regression\n",
    "- For that we will use the **Sigmoid function**:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/44.png)\n",
    "\n",
    "- We can zoom in on the function:\n",
    "\n",
    "![title](/notebooks/ML/Pictures/45.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
